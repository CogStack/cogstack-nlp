{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and using a custom tokenizer\n",
    "\n",
    "The overall process is quite simple\n",
    "- Implement `MutableDocument`, `MutableEntity`, and `MutableToken`\n",
    "  - These are tokenizer-specific wrappers\n",
    "  - In v1 this would have all been `spacy`-specific objects\n",
    "- Implement and extend `BaseTokenizer`\n",
    "- Register the component\n",
    "- Specify its use in the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will do things in steps\n",
    "\n",
    "### Start with the relevant imports and/or constance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, overload, Iterator, Optional, cast\n",
    "\n",
    "import re\n",
    "\n",
    "from medcat2.config.config import Config\n",
    "from medcat2.tokenizing.tokens import BaseDocument, BaseEntity, BaseToken\n",
    "from medcat2.tokenizing.tokens import MutableDocument, MutableEntity, MutableToken\n",
    "\n",
    "\n",
    "# define \"whitespace\"\n",
    "WHITESPACE = [' ', '\\t', '\\n']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we create a Token implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the documents, entities, tokens\n",
    "\n",
    "class Token:\n",
    "    STOP_TOKENS = set()  # nothing for now\n",
    "\n",
    "    # for base token\n",
    "    def __init__(self, doc: 'Document', text: str,\n",
    "                 token_index: int, start_index: int):\n",
    "        self._doc = doc\n",
    "        self._text = text\n",
    "        self._token_index = token_index\n",
    "        self._start_index = start_index\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return self._text\n",
    "\n",
    "    @property\n",
    "    def lower(self) -> str:\n",
    "        return self._text.lower()\n",
    "\n",
    "    @property\n",
    "    def text_versions(self) -> list[str]:\n",
    "        return [self.lemma, self.lower]\n",
    "\n",
    "    @property\n",
    "    def is_upper(self) -> bool:\n",
    "        return self._text.islower()\n",
    "\n",
    "    @property\n",
    "    def is_stop(self) -> bool:\n",
    "        return self.lower in self.STOP_TOKENS\n",
    "\n",
    "    @property\n",
    "    def char_index(self) -> int:\n",
    "        return self._start_index\n",
    "\n",
    "    @property\n",
    "    def index(self) -> int:\n",
    "        return self._token_index\n",
    "\n",
    "    @property\n",
    "    def text_with_ws(self) -> str:\n",
    "        end_index = self._start_index + len(self._text)\n",
    "        text = self._doc.text\n",
    "        if len(text) <= end_index:\n",
    "            next_char = ''\n",
    "        else:\n",
    "            next_char = text[end_index]\n",
    "        if next_char in WHITESPACE:\n",
    "            return self._text + next_char\n",
    "        return self._text\n",
    "\n",
    "    @property\n",
    "    def is_digit(self) -> bool:\n",
    "        return self._text.isdigit()\n",
    "\n",
    "    # for mutable token\n",
    "\n",
    "    @property\n",
    "    def base(self) -> BaseToken:\n",
    "        return self  # we implement both in the same class\n",
    "\n",
    "    _is_punctuation = False\n",
    "\n",
    "    @property\n",
    "    def is_punctuation(self) -> bool:\n",
    "        return self._is_punctuation\n",
    "\n",
    "    @is_punctuation.setter\n",
    "    def is_punctuation(self, val: bool) -> None:\n",
    "        self._is_punctuation = val\n",
    "\n",
    "    _to_skip = False\n",
    "\n",
    "    @property\n",
    "    def to_skip(self) -> bool:\n",
    "        return self._to_skip\n",
    "        \n",
    "\n",
    "    @to_skip.setter\n",
    "    def to_skip(self, new_val: bool) -> None:\n",
    "        self._to_skip = new_val\n",
    "\n",
    "    @property\n",
    "    def lemma(self) -> str:\n",
    "        return self.norm\n",
    "\n",
    "    @property\n",
    "    def tag(self) -> Optional[str]:\n",
    "        return None\n",
    "\n",
    "    @property\n",
    "    def norm(self) -> str:\n",
    "        \"\"\"The normalised text.\"\"\"\n",
    "        return self.lower\n",
    "\n",
    "    @norm.setter\n",
    "    def norm(self, value: str) -> None:\n",
    "        pass # nothing, for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving on to Entity and Document implementation\n",
    "\n",
    "The former needs to refer to the latter, so we want to define them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Entity:\n",
    "\n",
    "    def __init__(self, doc: 'Document',\n",
    "                 start_index: int, end_index: int):\n",
    "        self._doc = doc\n",
    "        self._start_index = start_index\n",
    "        self._end_index = end_index\n",
    "        # defaults\n",
    "        self.link_candidates: list[str] = []\n",
    "        self.context_similarity: float = 0.0\n",
    "        self.confidence: float = 0.0\n",
    "        self.cui = ''\n",
    "        self.id = -1  # TODO - what's the default?\n",
    "        self.detected_name = ''\n",
    "\n",
    "\n",
    "    # for base entity\n",
    "\n",
    "    @property\n",
    "    def start_index(self) -> int:\n",
    "        return self._start_index\n",
    "\n",
    "    @property\n",
    "    def end_index(self) -> int:\n",
    "        return self._end_index\n",
    "\n",
    "    @property\n",
    "    def start_char_index(self) -> int:\n",
    "        return self._doc[self._start_index].char_index\n",
    "\n",
    "    @property\n",
    "    def end_char_index(self) -> int:\n",
    "        if self._start_index == self.end_index:\n",
    "            return self._start_index\n",
    "        end_tkn = self._doc[self._end_index]\n",
    "        return end_tkn.char_index + len(end_tkn.text)\n",
    "\n",
    "    @property\n",
    "    def label(self) -> int:\n",
    "        return -1\n",
    "\n",
    "    @property\n",
    "    def text(self) -> str:\n",
    "        return self._doc.text[self.start_char_index: self.end_char_index]\n",
    "\n",
    "    def __iter__(self) -> Iterator[BaseToken]:\n",
    "        yield from self._doc._tokens[self.start_index: self.end_index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        if self._end_index == self._start_index:\n",
    "            return int(self.end_char_index > self.start_char_index)\n",
    "        return self.end_index - self.start_index\n",
    "\n",
    "    # for mutable entity\n",
    "\n",
    "    @property\n",
    "    def base(self) -> BaseEntity:\n",
    "        return self  # we implement both in same class\n",
    "\n",
    "    _ENTITY_INFO_PREFIX = \"ENTITY_INFO:\"\n",
    "\n",
    "    def set_addon_data(self, path: str, val: Any) -> None:\n",
    "        doc_dict = self._doc.get_addon_data(f\"{self._ENTITY_INFO_PREFIX}{path}\")\n",
    "        doc_dict[(self.start_index, self.end_index)] = val\n",
    "\n",
    "    def get_addon_data(self, path: str) -> Any:\n",
    "        doc_dict = self._doc.get_addon_data(f\"{self._ENTITY_INFO_PREFIX}{path}\")\n",
    "        return doc_dict[(self.start_index, self.end_index)]\n",
    "\n",
    "    @classmethod\n",
    "    def register_addon_path(cls, path: str, def_val: Any = None,\n",
    "                            force: bool = True) -> None:\n",
    "        # NOTE: saving with Document for persistence\n",
    "        Document.register_addon_path(f\"{cls._ENTITY_INFO_PREFIX}{path}\",\n",
    "            def_val=def_val, force=force)\n",
    "\n",
    "class Document:\n",
    "\n",
    "    def __init__(self, text: str, tokens: Optional[list[Token]]):\n",
    "        self.text = text\n",
    "        self._tokens = tokens or []\n",
    "        # filled by NER\n",
    "        self.all_ents: list[MutableEntity] = []\n",
    "        # filled by Linker\n",
    "        self.final_ents: list[MutableEntity] = []\n",
    "\n",
    "    @overload\n",
    "    def __getitem__(self, index: int) -> BaseToken:\n",
    "        pass\n",
    "\n",
    "    @overload\n",
    "    def __getitem__(self, index: slice) -> BaseEntity:\n",
    "        pass\n",
    "\n",
    "    def __iter__(self) -> Iterator[Token]:\n",
    "        yield from self._tokens\n",
    "\n",
    "    def isupper(self) -> bool:\n",
    "        return self.text.isupper()\n",
    "\n",
    "    # for mutable document\n",
    "\n",
    "    @property\n",
    "    def base(self) -> BaseDocument:\n",
    "        return self  # same calss again\n",
    "\n",
    "    def get_tokens(self, start_index: int, end_index: int\n",
    "                   ) -> list[MutableToken]:\n",
    "        tkns = []\n",
    "        for tkn in self:\n",
    "            if (tkn.char_index >= start_index and\n",
    "                    tkn.char_index <= end_index):\n",
    "                tkns.append(tkn)\n",
    "        return tkns\n",
    "\n",
    "\n",
    "    def set_addon_data(self, path: str, val: Any) -> None:\n",
    "        if not hasattr(self.__class__, path):\n",
    "            raise KeyError(f\"Path not registered with {self.__class__}: {path}\")\n",
    "        setattr(self, path, val)\n",
    "\n",
    "\n",
    "    def get_addon_data(self, path: str) -> Any:\n",
    "        if not hasattr(self.__class__, path):\n",
    "            raise KeyError(f\"Path not registered with {self.__class__}: {path}\")\n",
    "        return getattr(self, path)\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def register_addon_path(cls, path: str, def_val: Any = None,\n",
    "                            force: bool = True) -> None:\n",
    "        setattr(cls, path, def_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now the actual tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WhitespaceTokenizer:\n",
    "    SPLIT_PATTERN = re.compile(\"|\".join(map(re.escape, WHITESPACE)))\n",
    "\n",
    "    # methods needed for the BaseTokenizer protocol\n",
    "\n",
    "    def create_entity(self, doc: MutableDocument,\n",
    "                      token_start_index: int, token_end_index: int,\n",
    "                      label: str) -> MutableEntity:\n",
    "        return self.entity_from_tokens(doc[token_start_index: token_end_index])\n",
    "\n",
    "    def entity_from_tokens(self, tokens: list[MutableToken]) -> MutableEntity:\n",
    "        doc = cast(Token, tokens[0])._doc\n",
    "        start_index = tokens[0].base.index\n",
    "        end_index = tokens[-1].base.index\n",
    "        return Entity(doc, start_index, end_index)\n",
    "\n",
    "    def __call__(self, text: str) -> MutableDocument:\n",
    "        words = self.SPLIT_PATTERN.split(text)\n",
    "        doc = Document(text, tokens=None)\n",
    "        start_index = 0\n",
    "        for word_index, word in enumerate(words):\n",
    "            # NOTE: this only works if all the whitespace has length 1\n",
    "            if word:\n",
    "                # NOTE: ignore empty strings\n",
    "                doc._tokens.append(Token(doc, word, word_index, start_index))\n",
    "            # append start index by word length + 1 (for the splitter)\n",
    "            start_index += len(word) + 1 # splitter\n",
    "        return doc\n",
    "\n",
    "    def get_doc_class(self) -> type[MutableDocument]:\n",
    "        return Document\n",
    "\n",
    "    def get_entity_class(self) -> type[MutableEntity]:\n",
    "        return Entity\n",
    "\n",
    "    # for creation of class based on config\n",
    "    @classmethod\n",
    "    def get_init_args(cls, config: Config) -> list[Any]:\n",
    "        return []\n",
    "\n",
    "    @classmethod\n",
    "    def get_init_kwargs(cls, config: Config) -> dict[str, Any]:\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to register the tokenizer\n",
    "\n",
    "Now that we've got the implementation, we need to register it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered tokenizers: [('whitespace-tokenizer', 'WhitespaceTokenizer'), ('regex', 'RegexTokenizer'), ('spacy', 'SpacyTokenizer')]\n"
     ]
    }
   ],
   "source": [
    "from medcat2.tokenizing.tokenizers import register_tokenizer, list_available_tokenizers\n",
    "register_tokenizer(\"whitespace-tokenizer\", WhitespaceTokenizer)\n",
    "print(\"Registered tokenizers:\", list_available_tokenizers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can see if we can create one through the registry\n",
    "\n",
    "Normally, this will be automated through setting the tokenizer type in the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We've got one: <__main__.WhitespaceTokenizer object at 0x1129dc5b0>\n"
     ]
    }
   ],
   "source": [
    "from medcat2.tokenizing.tokenizers import create_tokenizer\n",
    "tokenizer = create_tokenizer(\"whitespace-tokenizer\")\n",
    "print(\"We've got one:\", tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we may want to tokenizer a simple sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: <__main__.Document object at 0x1129dc400>\n",
      "Tokens: [<__main__.Token object at 0x1129df640>, <__main__.Token object at 0x1129ddab0>, <__main__.Token object at 0x1129dc8b0>, <__main__.Token object at 0x1129dc8e0>, <__main__.Token object at 0x1129dfbe0>, <__main__.Token object at 0x1129dfc40>, <__main__.Token object at 0x1129de6b0>, <__main__.Token object at 0x1129df820>, <__main__.Token object at 0x1129de050>]\n",
      "Token texts: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'a', 'lazy', 'dog']\n",
      "0 @ 0 : 'The' vs 'The' \tW whitespace 'The '\n",
      "1 @ 4 : 'quick' vs 'quick' \tW whitespace 'quick '\n",
      "2 @ 10 : 'brown' vs 'brown' \tW whitespace 'brown '\n",
      "3 @ 16 : 'fox' vs 'fox' \tW whitespace 'fox '\n",
      "4 @ 20 : 'jumped' vs 'jumped' \tW whitespace 'jumped '\n",
      "5 @ 27 : 'over' vs 'over' \tW whitespace 'over '\n",
      "6 @ 32 : 'a' vs 'a' \tW whitespace 'a '\n",
      "7 @ 34 : 'lazy' vs 'lazy' \tW whitespace 'lazy '\n",
      "8 @ 39 : 'dog' vs 'dog' \tW whitespace 'dog'\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumped over a lazy dog\"\n",
    "doc = tokenizer(sentence)\n",
    "print(\"Doc:\", doc)\n",
    "tokens = list(doc)\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token texts:\", [tkn.base.text for tkn in tokens])\n",
    "# compare locations\n",
    "for tkn in doc:\n",
    "    start_char_index = tkn.base.char_index\n",
    "    end_char_index = start_char_index + len(tkn.base.text)\n",
    "    found_text = doc.base.text[start_char_index: end_char_index]\n",
    "    print(tkn.base.index, \"@\", start_char_index, \":\", repr(tkn.base.text), 'vs', repr(found_text),\n",
    "          \"\\tW whitespace\", repr(tkn.base.text_with_ws))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
